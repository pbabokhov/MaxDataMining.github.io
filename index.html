<!DOCTYPE html> 
<html>
  <head>
    <title>
      <h2>MaxDataMining</h2>
    </title>
  </head>
  <body>
    <ul>
      <li><a href="updates.html">Weekly Updates</a></li>
      <li>Problem Description</li>
      <li>Method Discussion</li>
       K-Nearest Neighbors, or KNN, is a supervised non-parametric lazy algorithm, first proposed by Evelyn Fix and J.L. Hodges in 1951 
       and improved upon by Thomas Cover in 1968. The basic premise of KNN is that. KNN can be used for both classification and regression.
       
       The most common measurement for KNN is Euclidean Distance. The math formula is given below:
       
                                D(xi, xj) = square root(sum from i=1 to k * (xik - xjk)^2)
       
       Where xi and xj are two training samples and k is the nearest neighbor to each training sample.
                                      
       KNN is non-parametric because there is no assumption about the data fits any sort of parameterized distribution. KNN is a popular algorithm to use 
       for machine learning because it is simple and because the training time is very quick, due to the method being a lazy algorithm. 
       That is, all the data is stored during the training phase, but the algorithm does not learn on the data until after classification. 
       This is good for cases when the distribution of the data is unknown, or in experiments when a quick classification is needed. 
       
       The status of KNN as a lazy algorithm also comes with disadvantages. One disadvantage is the calculation complexity. This stems from 
       KNN's retention of all the training data until testing. The algorithm calculates the distance between every sample within the training set
       This means that if there is a lot of data, the calculations of similarity between two training samples will take a long time. 
       Another disadvantage is that because KNN does not learn on the training set, it is not able to predict upon any newly-introduced data. 
       In addition, the training data This is why choosing a large enough value of k is important, because it will mitigate the the noise. Bu
       
       There are a number of improvements that have been proposed and implemented to make KNN 
      
      <li>Code</li>
    </ul>
</html>
